import json
import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from src.utils.llm_client import LLMClient

@dataclass
class ContentAnalysisSnapshot:
    """Snapshot of content analysis at a point in time"""
    timestamp: str
    topic: str
    overall_eeat_score: float
    experience_score: float
    expertise_score: float
    authoritativeness_score: float
    trustworthiness_score: float
    human_elements_score: float
    content_quality_score: float
    improvement_recommendations: List[str]
    business_context: Dict[str, Any]
    human_inputs_quality: float

class ContinuousImprovementTracker:
    def __init__(self):
        self.llm = LLMClient()
        self.analysis_history: List[ContentAnalysisSnapshot] = []
        
    def track_analysis(self, 
                      topic: str,
                      eeat_assessment: Dict[str, Any],
                      human_vs_ai_analysis: Dict[str, Any],
                      content_metrics: Dict[str, Any],
                      business_context: Dict[str, Any],
                      human_inputs: Dict[str, Any]) -> str:
        """Track a new content analysis and store for comparison"""
        
        # Create snapshot
        snapshot = ContentAnalysisSnapshot(
            timestamp=datetime.datetime.now().isoformat(),
            topic=topic,
            overall_eeat_score=eeat_assessment.get('overall_score', 0),
            experience_score=eeat_assessment.get('components', {}).get('experience', {}).get('score', 0),
            expertise_score=eeat_assessment.get('components', {}).get('expertise', {}).get('score', 0),
            authoritativeness_score=eeat_assessment.get('components', {}).get('authoritativeness', {}).get('score', 0),
            trustworthiness_score=eeat_assessment.get('components', {}).get('trustworthiness', {}).get('score', 0),
            human_elements_score=human_vs_ai_analysis.get('human_elements_score', 0),
            content_quality_score=content_metrics.get('quality_score', 0),
            improvement_recommendations=eeat_assessment.get('improvement_analysis', {}).get('immediate_actions', []),
            business_context=business_context,
            human_inputs_quality=self._assess_human_inputs_quality(human_inputs)
        )
        
        # Store snapshot
        snapshot_id = f"{topic}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.analysis_history.append(snapshot)
        
        return snapshot_id
    
    def generate_improvement_report(self, 
                                  current_snapshot_id: str,
                                  comparison_snapshot_id: Optional[str] = None) -> Dict[str, Any]:
        """Generate comprehensive improvement tracking report"""
        
        current_snapshot = self._get_snapshot_by_id(current_snapshot_id)
        if not current_snapshot:
            return {'error': 'Current snapshot not found'}
        
        # Compare with previous snapshot if available
        if comparison_snapshot_id:
            previous_snapshot = self._get_snapshot_by_id(comparison_snapshot_id)
        else:
            previous_snapshot = self._get_most_recent_previous_snapshot(current_snapshot.topic)
        
        if previous_snapshot:
            improvement_analysis = self._analyze_improvements(previous_snapshot, current_snapshot)
        else:
            improvement_analysis = self._generate_baseline_analysis(current_snapshot)
        
        # Generate comprehensive report
        report = {
            'improvement_summary': improvement_analysis,
            'current_performance': self._analyze_current_performance(current_snapshot),
            'trend_analysis': self._analyze_trends(current_snapshot.topic),
            'next_steps': self._generate_next_steps(current_snapshot, improvement_analysis),
            'roi_projection': self._project_roi(current_snapshot, improvement_analysis),
            'benchmark_comparison': self._compare_to_benchmarks(current_snapshot)
        }
        
        return report
    
    def _assess_human_inputs_quality(self, human_inputs: Dict[str, Any]) -> float:
        """Assess the quality of human inputs provided"""
        
        quality_score = 0.0
        max_score = 10.0
        
        # Check customer pain points quality
        pain_points = human_inputs.get('customer_pain_points', '')
        if len(pain_points) > 100:
            quality_score += 3.0
        elif len(pain_points) > 50:
            quality_score += 2.0
        elif len(pain_points) > 20:
            quality_score += 1.0
        
        # Check unique value proposition quality
        uvp = human_inputs.get('unique_value_prop', '')
        if len(uvp) > 80:
            quality_score += 2.5
        elif len(uvp) > 40:
            quality_score += 1.5
        elif len(uvp) > 10:
            quality_score += 0.5
        
        # Check business context completeness
        business_fields = ['industry', 'target_audience', 'business_type']
        completed_fields = sum(1 for field in business_fields if human_inputs.get(field))
        quality_score += (completed_fields / len(business_fields)) * 2.0
        
        # Check specificity and detail
        total_content = pain_points + ' ' + uvp
        if 'specific' in total_content.lower() or 'example' in total_content.lower():
            quality_score += 1.0
        
        # Check for authentic language (not generic)
        generic_phrases = ['we are the best', 'leading provider', 'innovative solutions']
        if not any(phrase in total_content.lower() for phrase in generic_phrases):
            quality_score += 1.5
        
        return min(max_score, quality_score)
    
    def _get_snapshot_by_id(self, snapshot_id: str) -> Optional[ContentAnalysisSnapshot]:
        """Get snapshot by ID"""
        for snapshot in self.analysis_history:
            if f"{snapshot.topic}_{datetime.datetime.fromisoformat(snapshot.timestamp).strftime('%Y%m%d_%H%M%S')}" == snapshot_id:
                return snapshot
        return None
    
    def _get_most_recent_previous_snapshot(self, topic: str) -> Optional[ContentAnalysisSnapshot]:
        """Get the most recent previous snapshot for the same topic"""
        topic_snapshots = [s for s in self.analysis_history if s.topic == topic]
        if len(topic_snapshots) >= 2:
            return sorted(topic_snapshots, key=lambda x: x.timestamp)[-2]
        return None
    
    def _analyze_improvements(self, previous: ContentAnalysisSnapshot, 
                            current: ContentAnalysisSnapshot) -> Dict[str, Any]:
        """Analyze improvements between two snapshots"""
        
        # Calculate score improvements
        improvements = {
            'overall_eeat': current.overall_eeat_score - previous.overall_eeat_score,
            'experience': current.experience_score - previous.experience_score,
            'expertise': current.expertise_score - previous.expertise_score,
            'authoritativeness': current.authoritativeness_score - previous.authoritativeness_score,
            'trustworthiness': current.trustworthiness_score - previous.trustworthiness_score,
            'human_elements': current.human_elements_score - previous.human_elements_score,
            'content_quality': current.content_quality_score - previous.content_quality_score,
            'human_inputs_quality': current.human_inputs_quality - previous.human_inputs_quality
        }
        
        # Analyze improvement patterns
        positive_improvements = {k: v for k, v in improvements.items() if v > 0}
        negative_changes = {k: v for k, v in improvements.items() if v < 0}
        
        # Calculate improvement velocity
        time_diff = datetime.datetime.fromisoformat(current.timestamp) - datetime.datetime.fromisoformat(previous.timestamp)
        days_between = time_diff.days + (time_diff.seconds / 86400)
        
        improvement_velocity = improvements['overall_eeat'] / max(days_between, 1) if days_between > 0 else 0
        
        # Determine improvement level
        overall_improvement = improvements['overall_eeat']
        if overall_improvement >= 2.0:
            improvement_level = 'excellent'
        elif overall_improvement >= 1.0:
            improvement_level = 'good'
        elif overall_improvement >= 0.5:
            improvement_level = 'moderate'
        elif overall_improvement >= 0:
            improvement_level = 'minimal'
        else:
            improvement_level = 'declining'
        
        return {
            'improvement_scores': improvements,
            'positive_improvements': positive_improvements,
            'areas_of_decline': negative_changes,
            'improvement_level': improvement_level,
            'improvement_velocity': round(improvement_velocity, 3),
            'time_period_days': round(days_between, 1),
            'key_achievements': self._identify_key_achievements(improvements),
            'improvement_insights': self._generate_improvement_insights(improvements, previous, current)
        }
    
    def _generate_baseline_analysis(self, snapshot: ContentAnalysisSnapshot) -> Dict[str, Any]:
        """Generate baseline analysis for first-time content"""
        
        return {
            'baseline_scores': {
                'overall_eeat': snapshot.overall_eeat_score,
                'experience': snapshot.experience_score,
                'expertise': snapshot.expertise_score,
                'authoritativeness': snapshot.authoritativeness_score,
                'trustworthiness': snapshot.trustworthiness_score,
                'human_elements': snapshot.human_elements_score
            },
            'improvement_level': 'baseline_established',
            'improvement_potential': round(10.0 - snapshot.overall_eeat_score, 1),
            'baseline_insights': self._generate_baseline_insights(snapshot)
        }
    
    def _identify_key_achievements(self, improvements: Dict[str, float]) -> List[str]:
        """Identify key achievements based on improvements"""
        
        achievements = []
        
        if improvements.get('overall_eeat', 0) >= 1.5:
            achievements.append('Significant E-E-A-T score improvement achieved')
        
        if improvements.get('trustworthiness', 0) >= 1.0:
            achievements.append('Major trustworthiness enhancement')
        
        if improvements.get('experience', 0) >= 1.0:
            achievements.append('Experience demonstration significantly improved')
        
        if improvements.get('human_elements', 0) >= 1.0:
            achievements.append('Human elements integration enhanced')
        
        if improvements.get('content_quality', 0) >= 1.0:
            achievements.append('Content quality standards elevated')
        
        # Check for balanced improvement
        component_scores = [
            improvements.get('experience', 0),
            improvements.get('expertise', 0),
            improvements.get('authoritativeness', 0),
            improvements.get('trustworthiness', 0)
        ]
        
        if all(score >= 0.5 for score in component_scores):
            achievements.append('Balanced improvement across all E-E-A-T components')
        
        return achievements or ['Foundation for improvement established']
    
    def _generate_improvement_insights(self, improvements: Dict[str, float],
                                     previous: ContentAnalysisSnapshot,
                                     current: ContentAnalysisSnapshot) -> List[str]:
        """Generate insights about the improvement patterns"""
        
        insights = []
        
        # Analyze strongest improvement area
        strongest_improvement = max(improvements.items(), key=lambda x: x[1])
        if strongest_improvement[1] > 0.5:
            insights.append(f"Strongest improvement in {strongest_improvement[0]}: +{strongest_improvement[1]:.1f}")
        
        # Analyze human inputs impact
        if improvements.get('human_inputs_quality', 0) > 0:
            insights.append("Improved human input quality directly correlates with E-E-A-T gains")
        
        # Identify patterns
        if improvements.get('experience', 0) > 0 and improvements.get('trustworthiness', 0) > 0:
            insights.append("Experience and trust improvements working synergistically")
        
        # Business context impact
        if current.business_context != previous.business_context:
            insights.append("Business context updates contributed to authority improvements")
        
        # Recommendation effectiveness
        if len(current.improvement_recommendations) < len(previous.improvement_recommendations):
            insights.append("Previous recommendations successfully addressed, fewer gaps remaining")
        
        return insights or ['Steady progress toward content excellence']
    
    def _generate_baseline_insights(self, snapshot: ContentAnalysisSnapshot) -> List[str]:
        """Generate insights for baseline content"""
        
        insights = []
        
        if snapshot.overall_eeat_score >= 7.0:
            insights.append("Strong baseline E-E-A-T foundation established")
        elif snapshot.overall_eeat_score >= 5.0:
            insights.append("Moderate E-E-A-T baseline with good improvement potential")
        else:
            insights.append("Significant E-E-A-T improvement opportunities identified")
        
        if snapshot.human_elements_score >= 7.0:
            insights.append("Excellent human element integration from the start")
        elif snapshot.human_elements_score >= 5.0:
            insights.append("Good human elements foundation to build upon")
        
        if snapshot.human_inputs_quality >= 7.0:
            insights.append("High-quality human inputs providing strong foundation")
        
        return insights
    
    def _analyze_current_performance(self, snapshot: ContentAnalysisSnapshot) -> Dict[str, Any]:
        """Analyze current performance levels"""
        
        # Performance categorization
        score = snapshot.overall_eeat_score
        if score >= 8.5:
            performance_level = 'exceptional'
            market_position = 'industry_leader'
        elif score >= 7.0:
            performance_level = 'excellent'
            market_position = 'strong_competitor'
        elif score >= 5.5:
            performance_level = 'good'
            market_position = 'market_participant'
        elif score >= 4.0:
            performance_level = 'fair'
            market_position = 'improvement_needed'
        else:
            performance_level = 'poor'
            market_position = 'significant_work_required'
        
        # Component analysis
        component_analysis = {
            'strongest_component': self._identify_strongest_component(snapshot),
            'weakest_component': self._identify_weakest_component(snapshot),
            'balanced_score': self._calculate_balance_score(snapshot),
            'improvement_priority': self._determine_improvement_priority(snapshot)
        }
        
        return {
            'performance_level': performance_level,
            'market_position': market_position,
            'current_score': score,
            'component_analysis': component_analysis,
            'readiness_assessment': self._assess_content_readiness(snapshot)
        }
    
    def _identify_strongest_component(self, snapshot: ContentAnalysisSnapshot) -> str:
        """Identify the strongest E-E-A-T component"""
        scores = {
            'experience': snapshot.experience_score,
            'expertise': snapshot.expertise_score,
            'authoritativeness': snapshot.authoritativeness_score,
            'trustworthiness': snapshot.trustworthiness_score
        }
        return max(scores.items(), key=lambda x: x[1])[0]
    
    def _identify_weakest_component(self, snapshot: ContentAnalysisSnapshot) -> str:
        """Identify the weakest E-E-A-T component"""
        scores = {
            'experience': snapshot.experience_score,
            'expertise': snapshot.expertise_score,
            'authoritativeness': snapshot.authoritativeness_score,
            'trustworthiness': snapshot.trustworthiness_score
        }
        return min(scores.items(), key=lambda x: x[1])[0]
    
    def _calculate_balance_score(self, snapshot: ContentAnalysisSnapshot) -> float:
        """Calculate how balanced the E-E-A-T scores are"""
        scores = [
            snapshot.experience_score,
            snapshot.expertise_score,
            snapshot.authoritativeness_score,
            snapshot.trustworthiness_score
        ]
        
        mean_score = sum(scores) / len(scores)
        variance = sum((score - mean_score) ** 2 for score in scores) / len(scores)
        
        # Higher balance score means more balanced (less variance)
        balance_score = max(0, 10 - variance)
        return round(balance_score, 1)
    
    def _determine_improvement_priority(self, snapshot: ContentAnalysisSnapshot) -> str:
        """Determine the priority area for improvement"""
        
        weakest = self._identify_weakest_component(snapshot)
        weakest_score = getattr(snapshot, f'{weakest}_score')
        
        if weakest_score < 4.0:
            return f'{weakest}_critical'
        elif weakest_score < 6.0:
            return f'{weakest}_high'
        else:
            return 'balanced_improvement'
    
    def _assess_content_readiness(self, snapshot: ContentAnalysisSnapshot) -> Dict[str, Any]:
        """Assess content readiness for publication/promotion"""
        
        readiness_checks = {
            'eeat_threshold': snapshot.overall_eeat_score >= 6.0,
            'human_elements': snapshot.human_elements_score >= 5.0,
            'content_quality': snapshot.content_quality_score >= 6.0,
            'trust_signals': snapshot.trustworthiness_score >= 6.0
        }
        
        readiness_score = sum(readiness_checks.values()) / len(readiness_checks) * 100
        
        if readiness_score >= 85:
            readiness_level = 'ready_for_publication'
        elif readiness_score >= 70:
            readiness_level = 'minor_improvements_needed'
        elif readiness_score >= 50:
            readiness_level = 'moderate_improvements_needed'
        else:
            readiness_level = 'significant_improvements_required'
        
        return {
            'readiness_level': readiness_level,
            'readiness_score': round(readiness_score, 1),
            'readiness_checks': readiness_checks,
            'blocking_issues': [k for k, v in readiness_checks.items() if not v]
        }
    
    def _analyze_trends(self, topic: str) -> Dict[str, Any]:
        """Analyze trends for the topic across all snapshots"""
        
        topic_snapshots = [s for s in self.analysis_history if s.topic == topic]
        if len(topic_snapshots) < 2:
            return {'message': 'Insufficient data for trend analysis'}
        
        # Sort by timestamp
        sorted_snapshots = sorted(topic_snapshots, key=lambda x: x.timestamp)
        
        # Calculate trends
        trends = {}
        metrics = ['overall_eeat_score', 'experience_score', 'expertise_score', 
                  'authoritativeness_score', 'trustworthiness_score', 'human_elements_score']
        
        for metric in metrics:
            values = [getattr(s, metric) for s in sorted_snapshots]
            
            # Simple trend calculation
            if len(values) >= 2:
                recent_avg = sum(values[-3:]) / min(3, len(values))
                early_avg = sum(values[:3]) / min(3, len(values))
                trend = recent_avg - early_avg
                
                trends[metric] = {
                    'direction': 'improving' if trend > 0.1 else 'declining' if trend < -0.1 else 'stable',
                    'magnitude': abs(trend),
                    'current_value': values[-1],
                    'best_value': max(values),
                    'trajectory': self._calculate_trajectory(values)
                }
        
        return {
            'trend_analysis': trends,
            'overall_trajectory': self._determine_overall_trajectory(trends),
            'trend_insights': self._generate_trend_insights(trends)
        }
    
    def _calculate_trajectory(self, values: List[float]) -> str:
        """Calculate trajectory based on recent values"""
        if len(values) < 3:
            return 'insufficient_data'
        
        recent_three = values[-3:]
        if recent_three[2] > recent_three[1] > recent_three[0]:
            return 'accelerating_improvement'
        elif recent_three[2] > recent_three[0]:
            return 'improving'
        elif recent_three[2] < recent_three[1] < recent_three[0]:
            return 'declining'
        else:
            return 'fluctuating'
    
    def _determine_overall_trajectory(self, trends: Dict[str, Any]) -> str:
        """Determine overall trajectory from individual trends"""
        improving_count = sum(1 for trend in trends.values() if trend.get('direction') == 'improving')
        total_trends = len(trends)
        
        if improving_count >= total_trends * 0.8:
            return 'strong_positive_trajectory'
        elif improving_count >= total_trends * 0.6:
            return 'positive_trajectory'
        elif improving_count >= total_trends * 0.4:
            return 'mixed_trajectory'
        else:
            return 'concerning_trajectory'
    
    def _generate_trend_insights(self, trends: Dict[str, Any]) -> List[str]:
        """Generate insights about trends"""
        insights = []
        
        # Find best performing trend
        best_trend = max(trends.items(), key=lambda x: x[1].get('magnitude', 0))
        if best_trend[1].get('direction') == 'improving':
            insights.append(f"Strongest improvement trend in {best_trend[0]}")
        
        # Check for consistent improvement
        improving_trends = [k for k, v in trends.items() if v.get('direction') == 'improving']
        if len(improving_trends) >= len(trends) * 0.7:
            insights.append("Consistent improvement across most metrics")
        
        # Identify acceleration
        accelerating = [k for k, v in trends.items() if v.get('trajectory') == 'accelerating_improvement']
        if accelerating:
            insights.append(f"Accelerating improvement in: {', '.join(accelerating)}")
        
        return insights or ['Trend analysis shows mixed results']
    
    def _generate_next_steps(self, snapshot: ContentAnalysisSnapshot, 
                           improvement_analysis: Dict[str, Any]) -> Dict[str, List[str]]:
        """Generate next steps based on current state and improvements"""
        
        next_steps = {
            'immediate_actions': [],
            'short_term_goals': [],
            'long_term_strategy': []
        }
        
        # Based on weakest component
        weakest = self._identify_weakest_component(snapshot)
        weakest_score = getattr(snapshot, f'{weakest}_score')
        
        if weakest_score < 5.0:
            next_steps['immediate_actions'].append(f'Focus on improving {weakest} - critical gap identified')
        
        # Based on improvement velocity
        velocity = improvement_analysis.get('improvement_velocity', 0)
        if velocity < 0.1:
            next_steps['immediate_actions'].append('Accelerate improvement efforts - velocity below target')
        
        # Based on human inputs quality
        if snapshot.human_inputs_quality < 6.0:
            next_steps['immediate_actions'].append('Enhance human input quality and specificity')
        
        # Short-term goals
        if snapshot.overall_eeat_score < 7.0:
            next_steps['short_term_goals'].append('Achieve E-E-A-T score of 7.0+ within 30 days')
        
        next_steps['short_term_goals'].append('Implement top 3 improvement recommendations')
        
        # Long-term strategy
        next_steps['long_term_strategy'].extend([
            'Build sustainable content authority',
            'Establish industry thought leadership',
            'Create comprehensive content ecosystem'
        ])
        
        return next_steps
    
    def _project_roi(self, snapshot: ContentAnalysisSnapshot, 
                    improvement_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Project ROI based on improvement trajectory"""
        
        current_score = snapshot.overall_eeat_score
        improvement_rate = improvement_analysis.get('improvement_velocity', 0.1)
        
        # Project scores at different timeframes
        projections = {
            '30_days': min(10.0, current_score + (improvement_rate * 30)),
            '90_days': min(10.0, current_score + (improvement_rate * 90)),
            '180_days': min(10.0, current_score + (improvement_rate * 180))
        }
        
        # Estimate performance improvements
        current_performance_multiplier = max(1.0, current_score / 5.0)
        
        roi_estimates = {}
        for timeframe, projected_score in projections.items():
            projected_multiplier = max(1.0, projected_score / 5.0)
            improvement_factor = projected_multiplier - current_performance_multiplier
            
            roi_estimates[timeframe] = {
                'projected_eeat_score': round(projected_score, 1),
                'performance_improvement': f"{improvement_factor * 100:.0f}%",
                'traffic_projection': f"{improvement_factor * 150:.0f}% increase",
                'engagement_projection': f"{improvement_factor * 200:.0f}% increase"
            }
        
        return {
            'roi_projections': roi_estimates,
            'investment_recommendation': self._determine_investment_level(current_score, improvement_rate),
            'break_even_estimate': self._estimate_break_even(improvement_rate)
        }
    
    def _determine_investment_level(self, current_score: float, improvement_rate: float) -> str:
        """Determine recommended investment level"""
        if current_score < 4.0:
            return 'high_investment_critical'
        elif current_score < 6.0 and improvement_rate > 0.1:
            return 'medium_investment_good_trajectory'
        elif current_score >= 7.0:
            return 'maintenance_investment'
        else:
            return 'moderate_investment_steady_improvement'
    
    def _estimate_break_even(self, improvement_rate: float) -> str:
        """Estimate break-even timeframe"""
        if improvement_rate >= 0.2:
            return '2-4 weeks'
        elif improvement_rate >= 0.1:
            return '4-8 weeks'
        elif improvement_rate >= 0.05:
            return '8-16 weeks'
        else:
            return '16+ weeks'
    
    def _compare_to_benchmarks(self, snapshot: ContentAnalysisSnapshot) -> Dict[str, Any]:
        """Compare current performance to industry benchmarks"""
        
        # Industry benchmarks (these would ideally come from a database)
        benchmarks = {
            'industry_average': {
                'overall_eeat': 5.5,
                'experience': 5.0,
                'expertise': 6.0,
                'authoritativeness': 5.0,
                'trustworthiness': 6.0
            },
            'top_quartile': {
                'overall_eeat': 7.5,
                'experience': 7.0,
                'expertise': 8.0,
                'authoritativeness': 7.0,
                'trustworthiness': 8.0
            },
            'industry_leaders': {
                'overall_eeat': 9.0,
                'experience': 8.5,
                'expertise': 9.5,
                'authoritativeness': 9.0,
                'trustworthiness': 9.5
            }
        }
        
        current_scores = {
            'overall_eeat': snapshot.overall_eeat_score,
            'experience': snapshot.experience_score,
            'expertise': snapshot.expertise_score,
            'authoritativeness': snapshot.authoritativeness_score,
            'trustworthiness': snapshot.trustworthiness_score
        }
        
        comparisons = {}
        for benchmark_level, benchmark_scores in benchmarks.items():
            comparisons[benchmark_level] = {}
            for metric, current_score in current_scores.items():
                benchmark_score = benchmark_scores.get(metric, 5.0)
                difference = current_score - benchmark_score
                
                comparisons[benchmark_level][metric] = {
                    'current': current_score,
                    'benchmark': benchmark_score,
                    'difference': round(difference, 1),
                    'status': 'above' if difference > 0.2 else 'below' if difference < -0.2 else 'at'
                }
        
        # Determine overall position
        overall_position = self._determine_market_position(current_scores, benchmarks)
        
        return {
            'benchmark_comparisons': comparisons,
            'market_position': overall_position,
            'competitive_gaps': self._identify_competitive_gaps(current_scores, benchmarks),
            'positioning_recommendations': self._generate_positioning_recommendations(overall_position)
        }
    
    def _determine_market_position(self, current_scores: Dict[str, float], 
                                 benchmarks: Dict[str, Dict[str, float]]) -> str:
        """Determine overall market position"""
        
        overall_score = current_scores['overall_eeat']
        
        if overall_score >= benchmarks['industry_leaders']['overall_eeat']:
            return 'industry_leader'
        elif overall_score >= benchmarks['top_quartile']['overall_eeat']:
            return 'top_quartile_performer'
        elif overall_score >= benchmarks['industry_average']['overall_eeat']:
            return 'above_average_performer'
        else:
            return 'below_average_performer'
    
    def _identify_competitive_gaps(self, current_scores: Dict[str, float],
                                 benchmarks: Dict[str, Dict[str, float]]) -> List[str]:
        """Identify competitive gaps"""
        
        gaps = []
        top_quartile = benchmarks['top_quartile']
        
        for metric, current_score in current_scores.items():
            if current_score < top_quartile.get(metric, 7.0) - 1.0:
                gaps.append(f'{metric}: {top_quartile[metric] - current_score:.1f} points below top quartile')
        
        return gaps or ['No significant competitive gaps identified']
    
    def _generate_positioning_recommendations(self, position: str) -> List[str]:
        """Generate positioning recommendations based on market position"""
        
        recommendations = {
            'industry_leader': [
                'Maintain leadership through continuous innovation',
                'Share thought leadership content',
                'Mentor others in the industry'
            ],
            'top_quartile_performer': [
                'Push toward industry leadership',
                'Build stronger brand recognition', 
                'Increase content authority signals'
            ],
            'above_average_performer': [
                'Focus on differentiation strategies',
                'Improve weakest E-E-A-T components',
                'Build stronger human elements'
            ],
            'below_average_performer': [
                'Immediate focus on fundamental improvements',
                'Invest in human expertise integration',
                'Address critical trust and authority gaps'
            ]
        }
        
        return recommendations.get(position, ['Continue steady improvement efforts'])
