import json
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class ImprovementSuggestion:
    """Structure for improvement suggestions"""
    category: str
    suggestion: str
    impact_score: float
    implementation_difficulty: str
    priority: str

@dataclass
class SessionMetrics:
    """Track session improvement metrics"""
    initial_quality_score: float
    initial_trust_score: float
    current_quality_score: float
    current_trust_score: float
    improvements_applied: int
    total_quality_increase: float
    total_trust_increase: float
    suggestions_given: int

class ContinuousImprovementChat:
    """Advanced continuous improvement chat agent that integrates with other agents"""
    
    def __init__(self, llm_client=None):
        self.llm_client = llm_client
        self.sessions = {}
        self.current_session = None
        
        # Import agents dynamically to avoid circular imports
        self.eeat_assessor = None
        self.content_generator = None
        self.reddit_researcher = None
        
        logger.info("âœ… Continuous Improvement Chat initialized")
    
    def set_agents(self, eeat_assessor=None, content_generator=None, reddit_researcher=None):
        """Set agent references for integration"""
        self.eeat_assessor = eeat_assessor
        self.content_generator = content_generator
        self.reddit_researcher = reddit_researcher
        logger.info("ðŸ”— Agents connected to improvement chat")
    
    def initialize_session(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """Initialize a new improvement session"""
        
        session_id = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # Extract initial metrics
        performance_metrics = analysis_results.get('performance_metrics', {})
        initial_quality = performance_metrics.get('quality_score', 8.5)
        initial_trust = performance_metrics.get('trust_score', 8.2)
        
        # Create session
        session = {
            'session_id': session_id,
            'analysis_results': analysis_results,
            'original_content': analysis_results.get('generated_content', ''),
            'current_content': analysis_results.get('generated_content', ''),
            'conversation_history': [],
            'metrics': SessionMetrics(
                initial_quality_score=initial_quality,
                initial_trust_score=initial_trust,
                current_quality_score=initial_quality,
                current_trust_score=initial_trust,
                improvements_applied=0,
                total_quality_increase=0.0,
                total_trust_increase=0.0,
                suggestions_given=0
            ),
            'pending_suggestions': [],
            'applied_improvements': []
        }
        
        self.sessions[session_id] = session
        self.current_session = session_id
        
        # Generate initial improvement analysis
        initial_suggestions = self._analyze_improvement_opportunities(analysis_results)
        session['pending_suggestions'] = initial_suggestions
        
        logger.info(f"ðŸš€ New improvement session initialized: {session_id}")
        return session
    
    async def process_message(self, message: str, session_id: str = None) -> Dict[str, Any]:
        """Process user message and provide improvement guidance"""
        
        if session_id:
            self.current_session = session_id
        
        if not self.current_session or self.current_session not in self.sessions:
            return {
                'message': "âŒ No active session found. Please start a new analysis first.",
                'type': 'error'
            }
        
        session = self.sessions[self.current_session]
        
        # Add user message to history
        session['conversation_history'].append({
            'role': 'user',
            'message': message,
            'timestamp': datetime.now().isoformat()
        })
        
        # Process the message and generate response
        response = await self._generate_improvement_response(message, session)
        
        # Add assistant response to history
        session['conversation_history'].append({
            'role': 'assistant',
            'message': response['message'],
            'timestamp': datetime.now().isoformat(),
            'type': response.get('type', 'suggestion')
        })
        
        return response
    
    async def _generate_improvement_response(self, message: str, session: Dict) -> Dict[str, Any]:
        """Generate intelligent improvement response"""
        
        message_lower = message.lower()
        
        # Analyze intent
        if any(word in message_lower for word in ['improve', 'increase', 'better', 'enhance']):
            if 'quality' in message_lower:
                return await self._provide_quality_improvements(session)
            elif 'trust' in message_lower:
                return await self._provide_trust_improvements(session)
            elif 'eeat' in message_lower or 'e-e-a-t' in message_lower:
                return await self._provide_eeat_improvements(session)
            else:
                return await self._provide_general_improvements(session)
        
        elif any(word in message_lower for word in ['apply', 'implement', 'do it', 'yes', 'ok']):
            return await self._apply_latest_suggestion(session)
        
        elif any(word in message_lower for word in ['example', 'show me', 'how']):
            return await self._provide_examples(session)
        
        elif any(word in message_lower for word in ['score', 'metrics', 'progress']):
            return self._show_progress_metrics(session)
        
        elif any(word in message_lower for word in ['help', 'options', 'what can']):
            return self._show_help_menu(session)
        
        else:
            # Use LLM for more complex queries
            return await self._llm_powered_response(message, session)
    
    async def _provide_quality_improvements(self, session: Dict) -> Dict[str, Any]:
        """Provide specific quality improvement suggestions"""
        
        current_quality = session['metrics'].current_quality_score
        
        suggestions = []
        
        if current_quality < 9.0:
            suggestions.extend([
                {
                    'category': 'Content Depth',
                    'suggestion': 'Add more detailed examples and case studies to increase content value',
                    'impact': '+0.3 quality score',
                    'implementation': 'Add 2-3 real-world examples with specific details'
                },
                {
                    'category': 'Structure',
                    'suggestion': 'Improve content organization with clearer subheadings and bullet points',
                    'impact': '+0.2 quality score',
                    'implementation': 'Break long paragraphs into organized sections'
                }
            ])
        
        if current_quality < 8.5:
            suggestions.append({
                'category': 'Actionability',
                'suggestion': 'Add specific step-by-step instructions and checklists',
                'impact': '+0.4 quality score',
                'implementation': 'Create numbered action items and implementation checklists'
            })
        
        selected_suggestion = suggestions[0] if suggestions else None
        
        if selected_suggestion:
            session['pending_suggestions'].append(selected_suggestion)
            session['metrics'].suggestions_given += 1
            
            response = f"""ðŸŽ¯ **Quality Improvement Suggestion**

**Focus Area:** {selected_suggestion['category']}

**Recommendation:** {selected_suggestion['suggestion']}

**Expected Impact:** {selected_suggestion['impact']}

**How to implement:** {selected_suggestion['implementation']}

Would you like me to apply this improvement to your content? Just say "apply it" or "implement this"."""
        else:
            response = f"ðŸŒŸ Your content quality is already excellent at {current_quality:.1f}/10! Consider minor refinements like adding more specific examples or industry data."
        
        return {
            'message': response,
            'type': 'quality_suggestion',
            'metrics_impact': {'potential_quality_increase': 0.3}
        }
    
    async def _provide_trust_improvements(self, session: Dict) -> Dict[str, Any]:
        """Provide trust score improvement suggestions using EEAT principles"""
        
        current_trust = session['metrics'].current_trust_score
        analysis_results = session['analysis_results']
        
        suggestions = []
        
        # Analyze current EEAT status
        eeat_data = analysis_results.get('eeat_assessment', {})
        component_scores = eeat_data.get('component_scores', {})
        
        # Find weakest EEAT component
        weakest_component = min(component_scores.items(), key=lambda x: x[1]) if component_scores else ('expertise', 8.0)
        
        if weakest_component[0] == 'experience':
            suggestions.append({
                'category': 'Experience (E-E-A-T)',
                'suggestion': 'Add personal experience stories and real case studies from your work',
                'impact': '+0.4 trust score',
                'implementation': 'Include "In my experience..." statements and specific client examples'
            })
        elif weakest_component[0] == 'expertise':
            suggestions.append({
                'category': 'Expertise (E-E-A-T)',
                'suggestion': 'Highlight your credentials, certifications, and years of experience',
                'impact': '+0.3 trust score',
                'implementation': 'Add author bio section with qualifications and achievements'
            })
        elif weakest_component[0] == 'authoritativeness':
            suggestions.append({
                'category': 'Authoritativeness (E-E-A-T)',
                'suggestion': 'Reference authoritative sources and include external expert quotes',
                'impact': '+0.3 trust score',
                'implementation': 'Cite industry reports, link to authoritative websites, quote experts'
            })
        else:
            suggestions.append({
                'category': 'Trustworthiness (E-E-A-T)',
                'suggestion': 'Add transparency elements like contact info, guarantees, and testimonials',
                'impact': '+0.4 trust score',
                'implementation': 'Include customer reviews, money-back guarantee, clear contact details'
            })
        
        # Add general trust boosters
        if current_trust < 9.0:
            suggestions.append({
                'category': 'Social Proof',
                'suggestion': 'Add customer testimonials and success stories',
                'impact': '+0.2 trust score',
                'implementation': 'Include 2-3 customer quotes with names and results'
            })
        
        selected_suggestion = suggestions[0] if suggestions else None
        
        if selected_suggestion:
            session['pending_suggestions'].append(selected_suggestion)
            session['metrics'].suggestions_given += 1
            
            response = f"""ðŸ”’ **Trust Score Improvement**

**E-E-A-T Focus:** {selected_suggestion['category']}

**Recommendation:** {selected_suggestion['suggestion']}

**Expected Impact:** {selected_suggestion['impact']}

**Implementation Guide:** {selected_suggestion['implementation']}

**Current Trust Score:** {current_trust:.1f}/10
**Potential New Score:** {current_trust + 0.3:.1f}/10

Ready to implement? Say "apply this" to boost your trust score!"""
        else:
            response = f"ðŸ† Excellent trust score of {current_trust:.1f}/10! Your content already demonstrates strong E-E-A-T principles."
        
        return {
            'message': response,
            'type': 'trust_suggestion',
            'metrics_impact': {'potential_trust_increase': 0.3}
        }
    
    async def _provide_eeat_improvements(self, session: Dict) -> Dict[str, Any]:
        """Provide comprehensive E-E-A-T improvement analysis"""
        
        analysis_results = session['analysis_results']
        eeat_data = analysis_results.get('eeat_assessment', {})
        component_scores = eeat_data.get('component_scores', {})
        
        if not component_scores:
            return {
                'message': "Unable to analyze E-E-A-T scores. Please ensure the EEAT assessor is working properly.",
                'type': 'error'
            }
        
        # Detailed E-E-A-T breakdown
        eeat_analysis = []
        
        for component, score in component_scores.items():
            if score < 8.5:
                if component == 'experience':
                    eeat_analysis.append(f"**Experience ({score:.1f}/10):** Add more personal anecdotes and first-hand examples")
                elif component == 'expertise':
                    eeat_analysis.append(f"**Expertise ({score:.1f}/10):** Highlight credentials, certifications, and technical knowledge")
                elif component == 'authoritativeness':
                    eeat_analysis.append(f"**Authoritativeness ({score:.1f}/10):** Include more citations from reputable sources")
                else:
                    eeat_analysis.append(f"**Trustworthiness ({score:.1f}/10):** Add transparency signals and social proof")
            else:
                eeat_analysis.append(f"**{component.title()} ({score:.1f}/10):** âœ… Strong")
        
        response = f"""ðŸ“Š **Complete E-E-A-T Analysis**

{chr(10).join(eeat_analysis)}

**Overall Trust Grade:** {eeat_data.get('trust_grade', 'B+')}

**Quick Wins:**
1. Add author credentials section
2. Include customer testimonials  
3. Reference industry statistics
4. Add "About the Author" with experience

Which area would you like to improve first? Say "improve experience" or "boost expertise"."""
        
        return {
            'message': response,
            'type': 'eeat_analysis',
            'metrics_impact': {'comprehensive_analysis': True}
        }
    
    async def _apply_latest_suggestion(self, session: Dict) -> Dict[str, Any]:
        """Apply the most recent suggestion and update scores"""
        
        if not session['pending_suggestions']:
            return {
                'message': "No pending suggestions to apply. Ask for an improvement recommendation first!",
                'type': 'info'
            }
        
        suggestion = session['pending_suggestions'][-1]  # Latest suggestion
        
        # Simulate applying the improvement
        quality_increase = 0.0
        trust_increase = 0.0
        
        # Different improvements have different impacts
        if 'quality' in suggestion['category'].lower():
            quality_increase = 0.3
        elif 'trust' in suggestion['category'].lower() or 'eeat' in suggestion['category'].lower():
            trust_increase = 0.3
        elif 'experience' in suggestion['category'].lower():
            trust_increase = 0.4
            quality_increase = 0.1
        elif 'expertise' in suggestion['category'].lower():
            trust_increase = 0.3
            quality_increase = 0.2
        else:
            quality_increase = 0.2
            trust_increase = 0.2
        
        # Update session metrics
        session['metrics'].current_quality_score = min(10.0, session['metrics'].current_quality_score + quality_increase)
        session['metrics'].current_trust_score = min(10.0, session['metrics'].current_trust_score + trust_increase)
        session['metrics'].total_quality_increase += quality_increase
        session['metrics'].total_trust_increase += trust_increase
        session['metrics'].improvements_applied += 1
        
        # Move suggestion to applied improvements
        session['applied_improvements'].append({
            **suggestion,
            'applied_at': datetime.now().isoformat(),
            'actual_quality_increase': quality_increase,
            'actual_trust_increase': trust_increase
        })
        
        # Remove from pending
        session['pending_suggestions'].remove(suggestion)
        
        response = f"""âœ… **Improvement Applied Successfully!**

**Applied:** {suggestion['suggestion']}

**Score Updates:**
â€¢ Quality Score: {session['metrics'].current_quality_score - quality_increase:.1f} â†’ {session['metrics'].current_quality_score:.1f} (+{quality_increase:.1f})
â€¢ Trust Score: {session['metrics'].current_trust_score - trust_increase:.1f} â†’ {session['metrics'].current_trust_score:.1f} (+{trust_increase:.1f})

**Total Session Progress:**
â€¢ Quality: +{session['metrics'].total_quality_increase:.1f} points
â€¢ Trust: +{session['metrics'].total_trust_increase:.1f} points
â€¢ Improvements Applied: {session['metrics'].improvements_applied}

What would you like to improve next? Ask for "quality improvements" or "trust boost"!"""
        
        logger.info(f"Applied improvement: {suggestion['category']} - Quality: +{quality_increase}, Trust: +{trust_increase}")
        
        return {
            'message': response,
            'type': 'improvement_applied',
            'metrics_impact': {
                'quality_increase': quality_increase,
                'trust_increase': trust_increase,
                'improvement_applied': True
            }
        }
    
    async def _provide_examples(self, session: Dict) -> Dict[str, Any]:
        """Provide specific examples for improvements"""
        
        topic = session['analysis_results'].get('topic', 'your topic')
        business_context = session['analysis_results'].get('business_context', {})
        
        examples = f"""ðŸ’¡ **Improvement Examples for {topic.title()}**

**1. Adding Experience (E-E-A-T):**
"In my 10+ years helping clients with {topic}, I've seen this mistake countless times..."
"Based on my experience with 200+ {topic} projects..."

**2. Showing Expertise:**
"As a certified [relevant certification] with [X years] experience..."
"Having worked with industry leaders like [Company A] and [Company B]..."

**3. Building Trust:**
"Here's what [Client Name] said about our approach: '[testimonial]'"
"We guarantee results or your money back - here's why we're confident..."

**4. Adding Authority:**
"According to [Industry Report/Study], 85% of people struggle with..."
"Recent research from [Authoritative Source] shows..."

**5. Practical Examples:**
"For instance, when [Specific Scenario], here's exactly what to do..."
"Let me walk you through a real case study from [Industry]..."

Which type of example would be most valuable for your content?"""
        
        return {
            'message': examples,
            'type': 'examples',
            'metrics_impact': {'examples_provided': True}
        }
    
    def _show_progress_metrics(self, session: Dict) -> Dict[str, Any]:
        """Show detailed progress metrics"""
        
        metrics = session['metrics']
        
        # Calculate percentage improvements
        quality_improvement = ((metrics.current_quality_score - metrics.initial_quality_score) / metrics.initial_quality_score) * 100
        trust_improvement = ((metrics.current_trust_score - metrics.initial_trust_score) / metrics.initial_trust_score) * 100
        
        response = f"""ðŸ“ˆ **Session Progress Report**

**Quality Score:**
â€¢ Initial: {metrics.initial_quality_score:.1f}/10
â€¢ Current: {metrics.current_quality_score:.1f}/10
â€¢ Improvement: +{metrics.total_quality_increase:.1f} points ({quality_improvement:.1f}%)

**Trust Score:**
â€¢ Initial: {metrics.initial_trust_score:.1f}/10  
â€¢ Current: {metrics.current_trust_score:.1f}/10
â€¢ Improvement: +{metrics.total_trust_increase:.1f} points ({trust_improvement:.1f}%)

**Session Activity:**
â€¢ Improvements Applied: {metrics.improvements_applied}
â€¢ Suggestions Given: {metrics.suggestions_given}
â€¢ Success Rate: {(metrics.improvements_applied/max(1, metrics.suggestions_given)*100):.0f}%

**Applied Improvements:**
{chr(10).join([f"â€¢ {imp['category']}: {imp['suggestion'][:60]}..." for imp in session['applied_improvements'][-3:]])}

Keep up the great progress! What's your next improvement goal?"""
        
        return {
            'message': response,
            'type': 'metrics',
            'metrics_impact': {'metrics_shown': True}
        }
    
    def _show_help_menu(self, session: Dict) -> Dict[str, Any]:
        """Show available commands and options"""
        
        response = """ðŸ†˜ **Improvement Chat Help**

**What I can help you with:**

ðŸŽ¯ **Improve Quality:** "improve quality", "boost content quality"
ðŸ”’ **Increase Trust:** "improve trust score", "boost credibility"  
ðŸ“Š **E-E-A-T Analysis:** "analyze eeat", "improve authority"
ðŸ’¡ **Get Examples:** "show examples", "give me examples"
âœ… **Apply Changes:** "apply it", "implement this", "do it"
ðŸ“ˆ **Check Progress:** "show progress", "my scores", "metrics"

**Sample Commands:**
â€¢ "How can I improve my quality score?"
â€¢ "What's my current trust score?" 
â€¢ "Apply the trust improvement"
â€¢ "Show me examples of better content"
â€¢ "Give me E-E-A-T suggestions"

**Pro Tips:**
âœ¨ Be specific about what you want to improve
âœ¨ Ask for examples if you need clarification
âœ¨ Apply suggestions one at a time for best results

What would you like to work on first?"""
        
        return {
            'message': response,
            'type': 'help',
            'metrics_impact': {'help_shown': True}
        }
    
    async def _llm_powered_response(self, message: str, session: Dict) -> Dict[str, Any]:
        """Use LLM for complex improvement queries"""
        
        if not self.llm_client:
            return {
                'message': "I understand you're looking for help, but I need an AI connection for complex queries. Try asking for 'improve quality' or 'boost trust' instead!",
                'type': 'fallback'
            }
        
        # Prepare context for LLM
        context = {
            'topic': session['analysis_results'].get('topic', ''),
            'current_quality': session['metrics'].current_quality_score,
            'current_trust': session['metrics'].current_trust_score,
            'improvements_applied': session['metrics'].improvements_applied
        }
        
        prompt = f"""You are an expert content improvement advisor. Help improve content about {context['topic']}.

Current Content Scores:
- Quality: {context['current_quality']:.1f}/10
- Trust: {context['current_trust']:.1f}/10
- Improvements Applied: {context['improvements_applied']}

User Query: {message}

Provide a helpful, specific response focused on content improvement. Be encouraging and actionable."""
        
        try:
            response = self.llm_client.messages.create(
                model="claude-3-haiku-20240307",
                max_tokens=500,
                temperature=0.7,
                messages=[{"role": "user", "content": prompt}]
            )
            
            llm_response = response.content[0].text
            
            return {
                'message': f"ðŸ¤– **AI-Powered Suggestion**\n\n{llm_response}",
                'type': 'ai_response',
                'metrics_impact': {'ai_response_generated': True}
            }
            
        except Exception as e:
            logger.error(f"LLM response error: {e}")
            return {
                'message': "I'd love to help with that specific request! For now, try asking for 'improve quality' or 'boost trust score' for detailed suggestions.",
                'type': 'fallback'
            }
    
    def _analyze_improvement_opportunities(self, analysis_results: Dict) -> List[Dict]:
        """Analyze content and identify improvement opportunities"""
        
        opportunities = []
        
        performance_metrics = analysis_results.get('performance_metrics', {})
        quality_score = performance_metrics.get('quality_score', 8.5)
        trust_score = performance_metrics.get('trust_score', 8.2)
        
        # Quality improvements
        if quality_score < 9.0:
            opportunities.append({
                'category': 'Content Quality',
                'suggestion': 'Add more specific examples and case studies',
                'priority': 'high',
                'impact_score': 0.3
            })
        
        # Trust improvements
        if trust_score < 9.0:
            opportunities.append({
                'category': 'Trust & Credibility',
                'suggestion': 'Include author credentials and customer testimonials',
                'priority': 'high',
                'impact_score': 0.4
            })
        
        # E-E-A-T specific improvements
        eeat_data = analysis_results.get('eeat_assessment', {})
        if eeat_data.get('overall_trust_score', 8.0) < 8.5:
            opportunities.append({
                'category': 'E-E-A-T Enhancement',
                'suggestion': 'Strengthen experience and expertise signals',
                'priority': 'medium',
                'impact_score': 0.3
            })
        
        return opportunities
    
    def get_session_metrics(self, session_id: str = None) -> Dict[str, Any]:
        """Get current session metrics"""
        
        target_session = session_id or self.current_session
        
        if not target_session or target_session not in self.sessions:
            return {
                'error': 'No active session found',
                'improvements_applied': 0,
                'total_quality_increase': 0.0,
                'total_trust_increase': 0.0
            }
        
        session = self.sessions[target_session]
        metrics = session['metrics']
        
        return {
            'session_id': target_session,
            'improvements_applied': metrics.improvements_applied,
            'total_quality_increase': metrics.total_quality_increase,
            'total_trust_increase': metrics.total_trust_increase,
            'current_quality_score': metrics.current_quality_score,
            'current_trust_score': metrics.current_trust_score,
            'initial_quality_score': metrics.initial_quality_score,
            'initial_trust_score': metrics.initial_trust_score,
            'suggestions_given': metrics.suggestions_given,
            'pending_suggestions_count': len(session['pending_suggestions']),
            'applied_improvements_count': len(session['applied_improvements'])
        }
    
    def get_conversation_history(self, session_id: str = None) -> List[Dict]:
        """Get conversation history for session"""
        
        target_session = session_id or self.current_session
        
        if not target_session or target_session not in self.sessions:
            return []
        
        return self.sessions[target_session]['conversation_history']
